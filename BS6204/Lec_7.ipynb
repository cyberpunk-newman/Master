{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lec_7",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMEwF+UUjeJor0ZJbD87oqi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyberpunk-newman/Master/blob/main/BS6204/Lec_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIBWtOZrT0II"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import yaml\n",
        "import requests\n",
        "\n",
        "from keras import Input, Model\n",
        "from keras.activations import softmax\n",
        "from keras.layers import Embedding,LSTM,Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNpCKMZFUvu-"
      },
      "source": [
        "# download data\n",
        "res = requests.get(\"https://uplevelsg.s3.ap-southeast-1.amazonaws.com/CommonAssets/chatbot_nlp.zip\")\n",
        "z = zipfile.ZipFile(io.BytesIO(res.content))\n",
        "z.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg6OhvBDV-rL",
        "outputId": "ed65db82-852e-4587-e317-c33bc1cfdf65"
      },
      "source": [
        "# get list of yml files\n",
        "dir_path ='chatbot_nlp/data'\n",
        "files_list = os.listdir(dir_path+os.sep) # get list of filenames\n",
        "files_list\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['history.yml',\n",
              " 'money.yml',\n",
              " 'psychology.yml',\n",
              " 'science.yml',\n",
              " 'emotion.yml',\n",
              " 'politics.yml',\n",
              " 'sports.yml',\n",
              " 'computers.yml',\n",
              " 'ai.yml',\n",
              " 'greetings.yml',\n",
              " 'botprofile.yml',\n",
              " 'trivia.yml',\n",
              " 'movies.yml',\n",
              " 'food.yml',\n",
              " 'health.yml',\n",
              " 'humor.yml',\n",
              " 'gossip.yml',\n",
              " 'literature.yml']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PMFU5o_WnBv",
        "outputId": "e7ab3878-f83a-4d5e-e108-d7f3dea74b59"
      },
      "source": [
        "# see what inside\n",
        "temp_yml = files_list[0]\n",
        "temp_stream = open(dir_path+\"/\"+temp_yml,'rb')\n",
        "temp_doc = yaml.safe_load(temp_stream)\n",
        "temp_doc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'categories': ['history'],\n",
              " 'conversations': [['tell me about the american civil war',\n",
              "   'do you think the south was right?'],\n",
              "  ['do you know about the american civil war',\n",
              "   'I am very interested in the war between the states.'],\n",
              "  ['What is history?',\n",
              "   \"History is the course of political, economic and military events over time, from the dawn of man to the age of AI.'\"],\n",
              "  ['what kind of history',\n",
              "   'I like to talk about the history of robots and computers.'],\n",
              "  ['are you interested in history',\n",
              "   'I am very interested in history, too. what period do you like?'],\n",
              "  ['explain history',\n",
              "   'history has two broad interpretations, depending on whether you accept the role of individuals as important or not.'],\n",
              "  ['who invented the lightbulb', 'thomas edison.'],\n",
              "  ['who invented the steam engine', 'james watt.']]}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "KNqfYj2DW5yv",
        "outputId": "f479a80f-bec0-4fb9-a1ba-330988b1627a"
      },
      "source": [
        "# create a list of questions\n",
        "questions=[]\n",
        "\n",
        "# create answers\n",
        "answers=[]\n",
        "\n",
        "# loop each yml\n",
        "for filepath in files_list:\n",
        "\n",
        "  # open stream\n",
        "  temp_stream = open(dir_path+\"/\"+filepath,'rb')\n",
        "\n",
        "  # load yml into variable safely\n",
        "  temp_doc = yaml.safe_load(temp_stream)\n",
        "\n",
        "  # retrieve conversations from yaml files\n",
        "  conversations = temp_doc['conversations']\n",
        "\n",
        "  for conversation in conversations:\n",
        "    # check conversations longer than two items\n",
        "    if len(conversation) > 2:\n",
        "      questions.append(conversations[0])\n",
        "\n",
        "      # add rest list as single answer\n",
        "\n",
        "      ans=''\n",
        "      for rep in conversations[1:]:\n",
        "        ans = ' '+ rep\n",
        "        answers.append(ans)\n",
        "\n",
        "    elif len(conversation) > 1:\n",
        "      questions.append(conversations[0])\n",
        "      answers.append(conversations[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-4d75dda02595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mrep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconversations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0manswers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "cfAqJNhpZRLT",
        "outputId": "ccaf9d15-1bf8-4fc8-b033-c68724a74b31"
      },
      "source": [
        "clean_answers=[]\n",
        "for i in range(len(answers)):\n",
        "  if type(answers[i]) ==str:\n",
        "    clean_answers.append(answers[i])\n",
        "\n",
        "  else:\n",
        "    questions.pop(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-89e6c98d9288>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'b' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJTAheSbZRmj"
      },
      "source": [
        "answers_with_tags=[]\n",
        "\n",
        "# add<start>  and <end> to sentenses\n",
        "\n",
        "for i in range(len(clean_answeres)):\n",
        "  answers_with_tags.append('<START>'+clean_answeres[i]+'<END>')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qudVj43UacCO"
      },
      "source": [
        "# data cleaning\n",
        "# declare regex\n",
        "\n",
        "target_regex ='~!@#$%^&*()_1234567890-=[]\\{}|:;\"<>?/.,'\n",
        "\n",
        "tokenizer = Tokenizer(filters=target_regex)\n",
        "tokenizer.fit_on_texts(question+answers_with_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U84ViV4ObK0E"
      },
      "source": [
        "# get vocab size of token\n",
        "VOCAB_SIZE = len(tokenizer.word_index)+1\n",
        "VOCAB_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "p2nVllB1bh6Z",
        "outputId": "bc1f6936-cb03-47d9-f2e9-e3f893c726f2"
      },
      "source": [
        "# turn questions into token\n",
        "tokenized_questions = tokenize.texts_to_sequences(questions_tags)\n",
        "\n",
        "# get max length of questions\n",
        "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
        "\n",
        "# get padded array words based on max length of questions\n",
        "encoder_input)data = pad_sequences(tokenized_questions,\n",
        "            maxlen = maxlen_questions,\n",
        "            padding='post')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-95474a1da6e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# turn questions into token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenized_questions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenized_questions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenize' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOttsoCtbzUe"
      },
      "source": [
        "# turn questions into token\n",
        "tokenized_answers = tokenize.texts_to_sequences(answers_tags)\n",
        "\n",
        "# get max length of questions\n",
        "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "\n",
        "# get padded array words based on max length of questions\n",
        "decoder_input_data = pad_sequences(tokenized_answers,\n",
        "            maxlen = maxlen_answers,\n",
        "            padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bCoYWradTCp"
      },
      "source": [
        "# prepareing one-hot encodede version of words\n",
        "# this is for decoder output\n",
        "for i in range(len(tokenized_answers)):\n",
        "  tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "\n",
        "padded_answers = pad_sequences(tokenized_answers,\n",
        "                maxlen = maxlen_answers,\n",
        "                padding='post')\n",
        "\n",
        "decoder_output_data = to_categorical(padded_answers,VOCAB_SIZE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRXD9LOLdtWy"
      },
      "source": [
        "# define encoder\n",
        "enc_inputs = Input(shape=(None,))\n",
        "\n",
        "# define embedding layer with input_dim, output_dim\n",
        "enc_embedding = Embedding(VOCAB_SIZE,200,mask_zero=True)(enc_inputs)\n",
        "\n",
        "#get hidden states and cell state from LSTM model\n",
        "_,state_h,state_c = LSTM(200,return_state=True)(enc_embedding)\n",
        "\n",
        "#store states in list\n",
        "enc_states=[state_h,state_c]\n",
        "\n",
        "#define the decoder\n",
        "dec_input=Input(shape=(None,))\n",
        "dec_embedding = Embedding(VOCAB_SIZE,200,mask_zero=True)(dec_inputs)\n",
        "dec_lstm = LSTM(200,return_state=True,return_sequences=True)\n",
        "\n",
        "#initialize LSTM layer with encoders status\n",
        "dec_outputs,_,_=dec_lstm(dec_embedding,initial_state=enc_states)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}